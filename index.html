<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="V-Skip: Efficient Multimodal Chain-of-Thought Compression via Dual-Path Anchoring. Achieves 2.9x speedup on Qwen2-VL while preventing Visual Amnesia.">
  <meta name="keywords" content="V-Skip, Multimodal LLM, Chain-of-Thought, Token Pruning, Efficient AI, Qwen2-VL, Hallucination, Visual Anchoring">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>V-Skip: Chain-of-Thought Compression Should Not Be Blind</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/XJTU_emblem.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://dongxu-zhang.github.io/">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://arxiv.org/abs/2601.13879">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>V-Skip Paper (ArXiv)</span>
            </a>
            <a class="navbar-item" href="https://dongxu-zhang.github.io/">
              <span class="icon"><i class="fas fa-user"></i></span>
              <span>Dongxu Zhang's Homepage</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Chain-of-Thought Compression Should Not Be Blind: <br> V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dongxu-zhang.github.io/">Dongxu Zhang</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/Issac-Sun">Yiding Sun</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://chengtan9907.github.io/">Cheng Tan</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LRLQFpUAAAAJ&hl=en">Wenbiao Yan</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="http://ningyangcasia.cn/">Ning Yang</a><sup>2,†</sup>,</span>
            <span class="author-block">
              <a href="https://gr.xjtu.edu.cn/web/zhujh">Jihua Zhu</a><sup>1,†</sup>,</span>
            <span class="author-block">
              <a href="https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-13/100.html">Haijun Zhang</a><sup>5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>State Key Laboratory of Human-Machine Hybrid Augmented Intelligence, XJTU,</span>
            <span class="author-block"><sup>2</sup>CASIA,</span>
            <span class="author-block"><sup>3</sup>Shanghai AI Lab,</span>
            <span class="author-block"><sup>4</sup>HITSZ,</span>
            <span class="author-block"><sup>5</sup>USTB</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block"><sup>*</sup>Equal Contribution</span>
              <span class="link-block"><sup>†</sup>Corresponding Author</span>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2601.13879"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2601.13879"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig1.png" class="interpolation-image" alt="Visual Amnesia Concept"/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">V-Skip</span> solves <b>Visual Amnesia</b>. 
        Standard text compression (middle) blindly prunes visually essential tokens (e.g., "red"), causing hallucinations. 
        V-Skip (bottom) preserves these visual anchors via Dual-Path scoring.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While Chain-of-Thought (CoT) reasoning significantly enhances the performance of 
            Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive 
            latency constraints. Current efforts to mitigate this via token compression often fail 
            by blindly applying text-centric metrics to multimodal contexts.
          </p>
          <p>
            We identify a critical failure mode termed <strong>Visual Amnesia</strong>, where 
            linguistically redundant tokens are erroneously pruned, severing the connection to the input image
            and leading to hallucinations. To address this, we introduce <strong>V-Skip</strong>, 
            a novel framework that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) 
            optimization problem.
          </p>
          <p>
            V-Skip employs a <strong>dual-path gating mechanism</strong> that weighs token importance through 
            both linguistic surprisal and cross-modal attention flow. This allows the model to identify and 
            rescue visually salient anchors that would otherwise be discarded. Extensive experiments on 
            Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a <strong>2.9× speedup</strong> 
            with negligible accuracy loss, outperforming other baselines by over 30% on the DocVQA benchmark.
          </p>
        </div>
      </div>
    </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            V-Skip reformulates token pruning as a <strong>Visual-Anchored Information Bottleneck (VA-IB)</strong> problem.
            The pipeline consists of three stages:
          </p>
          <ul>
            <li><strong>Stage 1:</strong> Data Generation using a frozen Teacher MLLM.</li>
            <li><strong>Stage 2:</strong> Dual-Path Filtering & Pruning, utilizing both <em>Linguistic Surprisal</em> and <em>Visual Attention Flow</em>.</li>
            <li><strong>Stage 3:</strong> Efficient Fine-tuning via LoRA distillation.</li>
          </ul>
        </div>
        <img src="./static/images/pipeline.png" class="interpolation-image" alt="V-Skip Architecture"/>
        <div class="content has-text-justified">
          <p>
            As shown above (Figure 2), our method automates the construction of efficient multimodal reasoners.
            Unlike standard text compression which discards tokens based only on text probability, V-Skip rescues
            visually salient tokens (anchors) to prevent hallucination.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Comparison</h2>
        
        <div class="content has-text-justified">
          <p>
            We compare V-Skip against standard text-centric pruning methods (e.g., LLMLingua-2).
            The example below (from DocVQA) demonstrates the <strong>Information Entropy Mismatch</strong>.
          </p>
        </div>

        <img src="./static/images/qualitative.png" class="interpolation-image" alt="Qualitative comparison on DocVQA"/>
        <img src="./static/images/qualitative2.png" class="interpolation-image" alt="Qualitative comparison on DocVQA"/>        
        <div class="content has-text-justified">
          <p>
            <br>
            In the invoice example, the key value <strong>"$45.20"</strong> has high linguistic perplexity (it looks like a random number to the LLM)
            but is visually grounded in the image.
            Standard methods (LLMLingua-2) prune it, leading to a wrong answer.
            <strong>V-Skip</strong> detects the high cross-modal attention and preserves the token, answering correctly.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhangvskip,
      title={Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring}, 
      author={Dongxu Zhang and Yiding Sun and Cheng Tan and Wenbiao Yan and Ning Yang and Jihua Zhu and Haijun Zhang},
      year={2026},
      eprint={2601.13879},
      archivePrefix={arXiv},
      primaryClass={cs.MM},
      url={https://arxiv.org/abs/2601.13879}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2601.13879">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/dongxu-zhang" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website utilizes the project page template from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
